{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f965561",
   "metadata": {},
   "source": [
    "# Stock Market Strategy Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db4aa4e",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be2d134",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "train = pd.read_csv('/train.csv',index_col = 0)\n",
    "test = pd.read_csv('/test.csv',index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2620be",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99629a52",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53e1ec2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad6965c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c807c554",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8936ed4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df2 = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3211f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(df2.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df708495",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(df.dtypes)\n",
    "print(df2.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba833e06",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(df['Open'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c80925",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(df['Volume'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56d4519",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(df2['Open'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2edd52",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(df2['Volume'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf29f210",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(df2['Open'].describe())\n",
    "print(df2['Volume'].describe())\n",
    "print(df2['Close'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509ec465",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(df2['Open'].corr(df2['Close']))\n",
    "print(df2['Open'].corr(df2['Volume']))\n",
    "print(df2['Open'].corr(df2['Volume']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7379d286",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14627be",
   "metadata": {},
   "source": [
    "## Autocorrelation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c219d0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import autocorrelation_plot\n",
    "autocorrelation_plot(df2['Close'])\n",
    "autocorrelation_plot(df2['Open'])\n",
    "autocorrelation_plot(df2['Volume'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d2c83e",
   "metadata": {},
   "source": [
    "## Scatter Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc51de",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(df2[['Open' , 'Close' , 'Volume']], alpha=0.2, figsize=(6, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d539e15",
   "metadata": {},
   "source": [
    "print(df2['Close'].skew())\n",
    "print(df2['Close'].kurtosis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba002afd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(df2['Open'].skew())\n",
    "print(df2['Open'].kurtosis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae469406",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(df2['Volume'].skew())\n",
    "print(df2['Volume'].kurtosis())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdab5af2",
   "metadata": {},
   "source": [
    "## ACF and PCF Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6525ade9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Load your time series data into a DataFrame\n",
    "# Assuming 'data' is your time series data\n",
    "data = pd.read_csv('/kaggle/input/dataset/train.csv')\n",
    "\n",
    "# Ensure the data is properly formatted with a datetime index\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.set_index('Date')\n",
    "\n",
    "# Select the numerical column you want to analyze\n",
    "numerical_column = data['Close']\n",
    "\n",
    "# Plot ACF (Autocorrelation Function)\n",
    "plt.figure(figsize=(12, 10))\n",
    "plot_acf(numerical_column, lags=40)  # Adjust 'lags' as needed\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.show()\n",
    "\n",
    "# Plot PACF (Partial Autocorrelation Function)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_pacf(numerical_column, lags=20)  # Adjust 'lags' as needed\n",
    "plt.title('Partial Autocorrelation Function (PACF)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32effdbc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "numerical_column = data['Open']\n",
    "\n",
    "# Plot ACF (Autocorrelation Function)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_acf(numerical_column, lags=40)  # Adjust 'lags' as needed\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.show()\n",
    "\n",
    "# Plot PACF (Partial Autocorrelation Function)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_pacf(numerical_column, lags=40)  # Adjust 'lags' as needed\n",
    "plt.title('Partial Autocorrelation Function (PACF)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09251c27",
   "metadata": {},
   "source": [
    "## ADF Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d7815c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "data = pd.read_csv('/kaggle/input/dataset/train.csv')\n",
    "\n",
    "# Ensure the data is properly formatted with a datetime index\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.set_index('Date')\n",
    "\n",
    "# Select the numerical column you want to test\n",
    "numerical_column = data['Close']\n",
    "\n",
    "# Perform the ADF test\n",
    "result = adfuller(numerical_column)\n",
    "\n",
    "# Extract the ADF test statistic and p-value\n",
    "adf_statistic = result[0]\n",
    "p_value = result[1]\n",
    "\n",
    "# Display the results\n",
    "print(f'ADF Statistic: {adf_statistic}')\n",
    "print(f'p-value: {p_value}')\n",
    "\n",
    "# Interpret the results\n",
    "if p_value <= 0.05:  # Common significance level of 0.05\n",
    "    print('Null hypothesis (non-stationary) rejected. Data is likely stationary.')\n",
    "else:\n",
    "    print('Null hypothesis not rejected. Data may be non-stationary.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3cdc43",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "data = pd.read_csv('/kaggle/input/dataset/train.csv')\n",
    "\n",
    "# Ensure the data is properly formatted with a datetime index\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.set_index('Date')\n",
    "\n",
    "# Select the numerical column you want to test\n",
    "numerical_column = data['Volume']\n",
    "\n",
    "# Perform the ADF test\n",
    "result = adfuller(numerical_column)\n",
    "\n",
    "# Extract the ADF test statistic and p-value\n",
    "adf_statistic = result[0]\n",
    "p_value = result[1]\n",
    "\n",
    "# Display the results\n",
    "print(f'ADF Statistic: {adf_statistic}')\n",
    "print(f'p-value: {p_value}')\n",
    "\n",
    "# Interpret the results\n",
    "if p_value <= 0.05:  # Common significance level of 0.05\n",
    "    print('Null hypothesis (non-stationary) rejected. Data is likely stationary.')\n",
    "else:\n",
    "    print('Null hypothesis not rejected. Data may be non-stationary.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c36aa1",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b9234f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "train_data = pd.read_csv('train.csv')\n",
    "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
    "train_data['Month'] = train_data['Date'].dt.month\n",
    "train_data['Day'] = train_data['Date'].dt.day\n",
    "train_data['Day_of_Week'] = train_data['Date'].dt.dayofweek\n",
    "train_data['Year'] = train_data['Date'].dt.year\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "columns_to_normalize = ['Open', 'Volume']\n",
    "scaler = MinMaxScaler()\n",
    "train_set_normalized = train_data.copy()  # Create a copy of the dataset to avoid altering the original\n",
    "normalized_values = scaler.fit_transform(train_set_normalized[columns_to_normalize])\n",
    "normalized_columns = [col + '_normalized' for col in columns_to_normalize]\n",
    "train_data[normalized_columns] = normalized_values\n",
    "num_lags = 3\n",
    "for i in range(1, num_lags + 1):\n",
    "    train_data[f'Open_lag_{i}'] = train_data['Open'].shift(i)\n",
    "    train_data[f'Volume_lag_{i}'] = train_data['Volume'].shift(i)\n",
    "for i in range(1, num_lags + 1):\n",
    "    train_data[f'Open_lag_{i}'].fillna(train_data[f'Open_lag_{i}'].mean(),inplace = True)\n",
    "    train_data[f'Volume_lag_{i}'].fillna(train_data[f'Open_lag_{i}'].mean(),inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "window_size = 5\n",
    "rolling_cols = ['Open', 'Volume']\n",
    "train_data['Open_rolling_mean'] = train_set_normalized['Open'].rolling(window=window_size).mean()\n",
    "train_data['Open_rolling_std'] = train_set_normalized['Open'].rolling(window=window_size).std()\n",
    "train_data['Volume_rolling_mean'] = train_set_normalized['Volume'].rolling(window=window_size).mean()\n",
    "train_data['Volume_rolling_std'] = train_set_normalized['Volume'].rolling(window=window_size).std()\n",
    "train_data['Open_rolling_mean'].fillna(train_data['Open_rolling_mean'].mean(), inplace=True)\n",
    "train_data['Open_rolling_std'].fillna(train_data['Open_rolling_std'].mean(), inplace=True)\n",
    "train_data['Volume_rolling_mean'].fillna(train_data['Volume_rolling_mean'].mean(), inplace=True)\n",
    "train_data['Volume_rolling_std'].fillna(train_data['Volume_rolling_std'].mean(), inplace=True)\n",
    "train_data['Open_Volume_interaction'] = train_data['Open'] * train_data['Volume']\n",
    "total_samples = len(train_data)\n",
    "train_size = int(total_samples * 0.8)\n",
    "train_set = train_data.iloc[:train_size]\n",
    "test_set = train_data.iloc[train_size:]\n",
    "p = 0\n",
    "d = 1\n",
    "q = 1\n",
    "\n",
    "exog_vars = ['Open', 'Volume', 'Open_Volume_interaction', 'Open_rolling_mean', 'Open_rolling_std', 'Volume_rolling_mean', 'Volume_rolling_std', 'Open_lag_1', 'Volume_lag_1', 'Open_lag_2', 'Volume_lag_2', 'Open_lag_3', 'Volume_lag_3']  # List of exogenous variables\n",
    "\n",
    "# Create and fit the ARIMAX model\n",
    "arimax_model = SARIMAX(train_data['Close'], order=(p, d, q), exog=train_data[exog_vars])\n",
    "arimax_model_fit = arimax_model.fit()\n",
    "\n",
    "# Assuming 'test_set' is the DataFrame you want to check\n",
    "\n",
    "# Forecast using the fitted model on the test data\n",
    "forecast = arimax_model_fit.forecast(steps=len(test_set), exog=test_set[exog_vars]).tolist()\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "test_actual = test_set['Close']\n",
    "smape = np.mean(np.abs(forecast - test_actual) / (np.abs(forecast) + np.abs(test_actual)))\n",
    "print(smape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d57163b",
   "metadata": {},
   "source": [
    "## Model Prediction for Closing Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d82aba2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Read the test data\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Perform similar preprocessing as done for the training data\n",
    "test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
    "test_data['Month'] = test_data['Date'].dt.month\n",
    "test_data['Day'] = test_data['Date'].dt.day\n",
    "test_data['Day_of_Week'] = test_data['Date'].dt.dayofweek\n",
    "test_data['Year'] = test_data['Date'].dt.year\n",
    "\n",
    "# Normalize columns\n",
    "scaler = MinMaxScaler()\n",
    "normalized_values = scaler.fit_transform(test_data[['Open', 'Volume']])\n",
    "normalized_columns = ['Open_normalized', 'Volume_normalized']\n",
    "test_data[normalized_columns] = normalized_values\n",
    "\n",
    "# Lag features\n",
    "num_lags = 3\n",
    "for i in range(1, num_lags + 1):\n",
    "    test_data[f'Open_lag_{i}'] = test_data['Open'].shift(i)\n",
    "    test_data[f'Volume_lag_{i}'] = test_data['Volume'].shift(i)\n",
    "\n",
    "for i in range(1, num_lags + 1):\n",
    "    test_data[f'Open_lag_{i}'].fillna(test_data[f'Open_lag_{i}'].mean(), inplace=True)\n",
    "    test_data[f'Volume_lag_{i}'].fillna(test_data[f'Open_lag_{i}'].mean(), inplace=True)\n",
    "\n",
    "# Rolling statistics\n",
    "window_size = 5\n",
    "test_data['Open_rolling_mean'] = test_data['Open'].rolling(window=window_size).mean()\n",
    "test_data['Open_rolling_std'] = test_data['Open'].rolling(window=window_size).std()\n",
    "test_data['Volume_rolling_mean'] = test_data['Volume'].rolling(window=window_size).mean()\n",
    "test_data['Volume_rolling_std'] = test_data['Volume'].rolling(window=window_size).std()\n",
    "\n",
    "test_data['Open_rolling_mean'].fillna(test_data['Open_rolling_mean'].mean(), inplace=True)\n",
    "test_data['Open_rolling_std'].fillna(test_data['Open_rolling_std'].mean(), inplace=True)\n",
    "test_data['Volume_rolling_mean'].fillna(test_data['Volume_rolling_mean'].mean(), inplace=True)\n",
    "test_data['Volume_rolling_std'].fillna(test_data['Volume_rolling_std'].mean(), inplace=True)\n",
    "\n",
    "# Interaction feature\n",
    "test_data['Open_Volume_interaction'] = test_data['Open'] * test_data['Volume']\n",
    "\n",
    "# Features used for prediction\n",
    "exog_vars = ['Open', 'Volume', 'Open_Volume_interaction', 'Open_rolling_mean', 'Open_rolling_std', 'Volume_rolling_mean', 'Volume_rolling_std', 'Open_lag_1', 'Volume_lag_1', 'Open_lag_2', 'Volume_lag_2', 'Open_lag_3', 'Volume_lag_3']\n",
    "\n",
    "forecast = arimax_model_fit.forecast(steps=len(test_data), exog=test_data[exog_vars])\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'date': test_data['Date'],\n",
    "    'Close': forecast.tolist()\n",
    "})\n",
    "close_values = forecast.tolist()\n",
    "\n",
    "\n",
    "# Save the updated 'test.csv' with predicted 'Close' values to a new file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.312534,
   "end_time": "2023-11-01T18:10:53.949363",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-01T18:10:39.636829",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
