{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f965561",
   "metadata": {},
   "source": [
    "# Stock Market Strategy Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db4aa4e",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be2d134",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "train = pd.read_csv('/train.csv',index_col = 0)\n",
    "test = pd.read_csv('/test.csv',index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2620be",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99629a52",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53e1ec2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad6965c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c807c554",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8936ed4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df2 = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3211f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(df2.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df708495",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(df.dtypes)\n",
    "print(df2.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba833e06",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(df['Open'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c80925",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(df['Volume'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56d4519",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(df2['Open'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2edd52",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(df2['Volume'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf29f210",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(df2['Open'].describe())\n",
    "print(df2['Volume'].describe())\n",
    "print(df2['Close'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509ec465",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(df2['Open'].corr(df2['Close']))\n",
    "print(df2['Open'].corr(df2['Volume']))\n",
    "print(df2['Open'].corr(df2['Volume']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7379d286",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14627be",
   "metadata": {},
   "source": [
    "## Autocorrelation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c219d0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import autocorrelation_plot\n",
    "autocorrelation_plot(df2['Close'])\n",
    "autocorrelation_plot(df2['Open'])\n",
    "autocorrelation_plot(df2['Volume'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d2c83e",
   "metadata": {},
   "source": [
    "## Scatter Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc51de",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(df2[['Open' , 'Close' , 'Volume']], alpha=0.2, figsize=(6, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d539e15",
   "metadata": {},
   "source": [
    "print(df2['Close'].skew())\n",
    "print(df2['Close'].kurtosis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba002afd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(df2['Open'].skew())\n",
    "print(df2['Open'].kurtosis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae469406",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(df2['Volume'].skew())\n",
    "print(df2['Volume'].kurtosis())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdab5af2",
   "metadata": {},
   "source": [
    "## ACF and PCF Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6525ade9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Load your time series data into a DataFrame\n",
    "# Assuming 'data' is your time series data\n",
    "data = pd.read_csv('/kaggle/input/dataset/train.csv')\n",
    "\n",
    "# Ensure the data is properly formatted with a datetime index\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.set_index('Date')\n",
    "\n",
    "# Select the numerical column you want to analyze\n",
    "numerical_column = data['Close']\n",
    "\n",
    "# Plot ACF (Autocorrelation Function)\n",
    "plt.figure(figsize=(12, 10))\n",
    "plot_acf(numerical_column, lags=40)  # Adjust 'lags' as needed\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.show()\n",
    "\n",
    "# Plot PACF (Partial Autocorrelation Function)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_pacf(numerical_column, lags=20)  # Adjust 'lags' as needed\n",
    "plt.title('Partial Autocorrelation Function (PACF)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32effdbc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "numerical_column = data['Open']\n",
    "\n",
    "# Plot ACF (Autocorrelation Function)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_acf(numerical_column, lags=40)  # Adjust 'lags' as needed\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.show()\n",
    "\n",
    "# Plot PACF (Partial Autocorrelation Function)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_pacf(numerical_column, lags=40)  # Adjust 'lags' as needed\n",
    "plt.title('Partial Autocorrelation Function (PACF)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09251c27",
   "metadata": {},
   "source": [
    "## ADF Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d7815c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "data = pd.read_csv('/kaggle/input/dataset/train.csv')\n",
    "\n",
    "# Ensure the data is properly formatted with a datetime index\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.set_index('Date')\n",
    "\n",
    "# Select the numerical column you want to test\n",
    "numerical_column = data['Close']\n",
    "\n",
    "# Perform the ADF test\n",
    "result = adfuller(numerical_column)\n",
    "\n",
    "# Extract the ADF test statistic and p-value\n",
    "adf_statistic = result[0]\n",
    "p_value = result[1]\n",
    "\n",
    "# Display the results\n",
    "print(f'ADF Statistic: {adf_statistic}')\n",
    "print(f'p-value: {p_value}')\n",
    "\n",
    "# Interpret the results\n",
    "if p_value <= 0.05:  # Common significance level of 0.05\n",
    "    print('Null hypothesis (non-stationary) rejected. Data is likely stationary.')\n",
    "else:\n",
    "    print('Null hypothesis not rejected. Data may be non-stationary.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3cdc43",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "data = pd.read_csv('/kaggle/input/dataset/train.csv')\n",
    "\n",
    "# Ensure the data is properly formatted with a datetime index\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.set_index('Date')\n",
    "\n",
    "# Select the numerical column you want to test\n",
    "numerical_column = data['Volume']\n",
    "\n",
    "# Perform the ADF test\n",
    "result = adfuller(numerical_column)\n",
    "\n",
    "# Extract the ADF test statistic and p-value\n",
    "adf_statistic = result[0]\n",
    "p_value = result[1]\n",
    "\n",
    "# Display the results\n",
    "print(f'ADF Statistic: {adf_statistic}')\n",
    "print(f'p-value: {p_value}')\n",
    "\n",
    "# Interpret the results\n",
    "if p_value <= 0.05:  # Common significance level of 0.05\n",
    "    print('Null hypothesis (non-stationary) rejected. Data is likely stationary.')\n",
    "else:\n",
    "    print('Null hypothesis not rejected. Data may be non-stationary.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c36aa1",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b9234f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "train_data = pd.read_csv('train.csv')\n",
    "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
    "train_data['Month'] = train_data['Date'].dt.month\n",
    "train_data['Day'] = train_data['Date'].dt.day\n",
    "train_data['Day_of_Week'] = train_data['Date'].dt.dayofweek\n",
    "train_data['Year'] = train_data['Date'].dt.year\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "columns_to_normalize = ['Open', 'Volume']\n",
    "scaler = MinMaxScaler()\n",
    "train_set_normalized = train_data.copy()  # Create a copy of the dataset to avoid altering the original\n",
    "normalized_values = scaler.fit_transform(train_set_normalized[columns_to_normalize])\n",
    "normalized_columns = [col + '_normalized' for col in columns_to_normalize]\n",
    "train_data[normalized_columns] = normalized_values\n",
    "num_lags = 3\n",
    "for i in range(1, num_lags + 1):\n",
    "    train_data[f'Open_lag_{i}'] = train_data['Open'].shift(i)\n",
    "    train_data[f'Volume_lag_{i}'] = train_data['Volume'].shift(i)\n",
    "for i in range(1, num_lags + 1):\n",
    "    train_data[f'Open_lag_{i}'].fillna(train_data[f'Open_lag_{i}'].mean(),inplace = True)\n",
    "    train_data[f'Volume_lag_{i}'].fillna(train_data[f'Open_lag_{i}'].mean(),inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "window_size = 5\n",
    "rolling_cols = ['Open', 'Volume']\n",
    "train_data['Open_rolling_mean'] = train_set_normalized['Open'].rolling(window=window_size).mean()\n",
    "train_data['Open_rolling_std'] = train_set_normalized['Open'].rolling(window=window_size).std()\n",
    "train_data['Volume_rolling_mean'] = train_set_normalized['Volume'].rolling(window=window_size).mean()\n",
    "train_data['Volume_rolling_std'] = train_set_normalized['Volume'].rolling(window=window_size).std()\n",
    "train_data['Open_rolling_mean'].fillna(train_data['Open_rolling_mean'].mean(), inplace=True)\n",
    "train_data['Open_rolling_std'].fillna(train_data['Open_rolling_std'].mean(), inplace=True)\n",
    "train_data['Volume_rolling_mean'].fillna(train_data['Volume_rolling_mean'].mean(), inplace=True)\n",
    "train_data['Volume_rolling_std'].fillna(train_data['Volume_rolling_std'].mean(), inplace=True)\n",
    "train_data['Open_Volume_interaction'] = train_data['Open'] * train_data['Volume']\n",
    "total_samples = len(train_data)\n",
    "train_size = int(total_samples * 0.8)\n",
    "train_set = train_data.iloc[:train_size]\n",
    "test_set = train_data.iloc[train_size:]\n",
    "p = 0\n",
    "d = 1\n",
    "q = 1\n",
    "\n",
    "exog_vars = ['Open', 'Volume', 'Open_Volume_interaction', 'Open_rolling_mean', 'Open_rolling_std', 'Volume_rolling_mean', 'Volume_rolling_std', 'Open_lag_1', 'Volume_lag_1', 'Open_lag_2', 'Volume_lag_2', 'Open_lag_3', 'Volume_lag_3']  # List of exogenous variables\n",
    "\n",
    "# Create and fit the ARIMAX model\n",
    "arimax_model = SARIMAX(train_data['Close'], order=(p, d, q), exog=train_data[exog_vars])\n",
    "arimax_model_fit = arimax_model.fit()\n",
    "\n",
    "# Assuming 'test_set' is the DataFrame you want to check\n",
    "\n",
    "# Forecast using the fitted model on the test data\n",
    "forecast = arimax_model_fit.forecast(steps=len(test_set), exog=test_set[exog_vars]).tolist()\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "test_actual = test_set['Close']\n",
    "smape = np.mean(np.abs(forecast - test_actual) / (np.abs(forecast) + np.abs(test_actual)))\n",
    "print(smape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d57163b",
   "metadata": {},
   "source": [
    "## Model Prediction for Closing Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d82aba2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Read the test data\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Perform similar preprocessing as done for the training data\n",
    "test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
    "test_data['Month'] = test_data['Date'].dt.month\n",
    "test_data['Day'] = test_data['Date'].dt.day\n",
    "test_data['Day_of_Week'] = test_data['Date'].dt.dayofweek\n",
    "test_data['Year'] = test_data['Date'].dt.year\n",
    "\n",
    "# Normalize columns\n",
    "scaler = MinMaxScaler()\n",
    "normalized_values = scaler.fit_transform(test_data[['Open', 'Volume']])\n",
    "normalized_columns = ['Open_normalized', 'Volume_normalized']\n",
    "test_data[normalized_columns] = normalized_values\n",
    "\n",
    "# Lag features\n",
    "num_lags = 3\n",
    "for i in range(1, num_lags + 1):\n",
    "    test_data[f'Open_lag_{i}'] = test_data['Open'].shift(i)\n",
    "    test_data[f'Volume_lag_{i}'] = test_data['Volume'].shift(i)\n",
    "\n",
    "for i in range(1, num_lags + 1):\n",
    "    test_data[f'Open_lag_{i}'].fillna(test_data[f'Open_lag_{i}'].mean(), inplace=True)\n",
    "    test_data[f'Volume_lag_{i}'].fillna(test_data[f'Open_lag_{i}'].mean(), inplace=True)\n",
    "\n",
    "# Rolling statistics\n",
    "window_size = 5\n",
    "test_data['Open_rolling_mean'] = test_data['Open'].rolling(window=window_size).mean()\n",
    "test_data['Open_rolling_std'] = test_data['Open'].rolling(window=window_size).std()\n",
    "test_data['Volume_rolling_mean'] = test_data['Volume'].rolling(window=window_size).mean()\n",
    "test_data['Volume_rolling_std'] = test_data['Volume'].rolling(window=window_size).std()\n",
    "\n",
    "test_data['Open_rolling_mean'].fillna(test_data['Open_rolling_mean'].mean(), inplace=True)\n",
    "test_data['Open_rolling_std'].fillna(test_data['Open_rolling_std'].mean(), inplace=True)\n",
    "test_data['Volume_rolling_mean'].fillna(test_data['Volume_rolling_mean'].mean(), inplace=True)\n",
    "test_data['Volume_rolling_std'].fillna(test_data['Volume_rolling_std'].mean(), inplace=True)\n",
    "\n",
    "# Interaction feature\n",
    "test_data['Open_Volume_interaction'] = test_data['Open'] * test_data['Volume']\n",
    "\n",
    "# Features used for prediction\n",
    "exog_vars = ['Open', 'Volume', 'Open_Volume_interaction', 'Open_rolling_mean', 'Open_rolling_std', 'Volume_rolling_mean', 'Volume_rolling_std', 'Open_lag_1', 'Volume_lag_1', 'Open_lag_2', 'Volume_lag_2', 'Open_lag_3', 'Volume_lag_3']\n",
    "\n",
    "forecast = arimax_model_fit.forecast(steps=len(test_data), exog=test_data[exog_vars])\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'date': test_data['Date'],\n",
    "    'Close': forecast.tolist()\n",
    "})\n",
    "close_values = forecast.tolist()\n",
    "\n",
    "\n",
    "# Save the updated 'test.csv' with predicted 'Close' values to a new file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87f3cd7",
   "metadata": {},
   "source": [
    "## Strategy Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8491495",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your train and test datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "train_data['Strategy_Encoded'] = train_data['Strategy'].map({'Buy': 0, 'Sell': 1, 'Hold': 2})\n",
    "\n",
    "# Calculate the moving average (you can adjust the window size as needed)\n",
    "window_size = 3\n",
    "train_data['Moving_Avg'] = train_data['Close'].rolling(window=window_size).mean()\n",
    "# Function to calculate RSI\n",
    "def calculate_bollinger_bands(data, window=20, num_std_dev=2):\n",
    "    data['SMA'] = data['Close'].rolling(window=window).mean()\n",
    "    data['STD'] = data['Close'].rolling(window=window).std()\n",
    "    data['UpperBand'] = data['SMA'] + (data['STD'] * num_std_dev)\n",
    "    data['LowerBand'] = data['SMA'] - (data['STD'] * num_std_dev)\n",
    "    return data\n",
    "\n",
    "def calculate_obv(data):\n",
    "    data['Daily_Return'] = data['Close'].diff()\n",
    "    data['Direction'] = (data['Daily_Return'] >= 0).astype(int)\n",
    "    data['Volume_Signed'] = data['Volume'] * data['Direction']\n",
    "    data['OBV'] = data['Volume_Signed'].cumsum()\n",
    "    data.drop(['Daily_Return', 'Direction', 'Volume_Signed'], axis=1, inplace=True)  # Drop intermediate columns\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = calculate_obv(train_data)\n",
    "\n",
    "train_data = calculate_bollinger_bands(train_data)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_rsi(data, window_length):\n",
    "    delta = data['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window_length).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window_length).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "window_length = 10\n",
    "train_data['RSI'] = calculate_rsi(train_data, window_length)\n",
    "\n",
    "def calculate_vwap(data):\n",
    "    data['TP'] = (data['High'] + data['Low'] + data['Close']) / 3\n",
    "    data['TPV'] = data['TP'] * data['Volume']\n",
    "    data['CumulativeTPV'] = data['TPV'].cumsum()\n",
    "    data['CumulativeVolume'] = data['Volume'].cumsum()\n",
    "    data['VWAP'] = data['CumulativeTPV'] / data['CumulativeVolume']\n",
    "    return data['VWAP']\n",
    "\n",
    "\n",
    "\n",
    "train_data['High'] = train_data['Close'].rolling(window=window_size).max()\n",
    "train_data['Low'] = train_data['Close'].rolling(window=window_size).min()\n",
    "train_data['VWAP'] = calculate_vwap(train_data)\n",
    "train_data['EMA_12'] = train_data['Close'].ewm(span=12, adjust=False).mean()\n",
    "train_data['EMA_26'] = train_data['Close'].ewm(span=26, adjust=False).mean()\n",
    "train_data['MACD'] = train_data['EMA_12'] - train_data['EMA_26']\n",
    "# Define features and target\n",
    "features = ['Open', 'Volume', 'Close', 'Moving_Avg','RSI','VWAP','MACD','UpperBand','LowerBand','OBV']\n",
    "target = ['Strategy_Encoded']\n",
    "\n",
    "X = train_data[features]\n",
    "y = train_data[target]\n",
    "\n",
    "# Split the train data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the XGBoost classifier\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test).tolist()\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on the test subset: {accuracy}\")\n",
    "\n",
    "# Calculate RSI for test_data\n",
    "test_data['Close'] = close_values\n",
    "test_data['Moving_Avg'] = test_data['Close'].rolling(window=window_size).mean()\n",
    "test_data['RSI'] = calculate_rsi(test_data, window_length)\n",
    "test_data['High'] = train_data['Close'].rolling(window=window_size).max()\n",
    "test_data['Low'] = train_data['Close'].rolling(window=window_size).min()\n",
    "test_data['VWAP'] = calculate_vwap(test_data)\n",
    "test_data['EMA_12'] = test_data['Close'].ewm(span=12, adjust=False).mean()\n",
    "test_data['EMA_26'] = test_data['Close'].ewm(span=26, adjust=False).mean()\n",
    "test_data['MACD'] = test_data['EMA_12'] - test_data['EMA_26']\n",
    "test_data = calculate_bollinger_bands(test_data)\n",
    "test_data = calculate_obv(test_data)\n",
    "X_test_data = test_data[features]\n",
    "predicted_strategies = model.predict(X_test_data).tolist()\n",
    "\n",
    "# Mapping function to convert integer predictions to string labels\n",
    "def map_to_string(prediction):\n",
    "    if prediction == 0:\n",
    "        return 'Buy'\n",
    "    elif prediction == 1:\n",
    "        return 'Sell'\n",
    "    elif prediction == 2:\n",
    "        return 'Hold'\n",
    "    else:\n",
    "        return 'Unknown'  # Handle other cases if needed\n",
    "\n",
    "# Applying the mapping function to convert integer predictions to string labels\n",
    "predicted_labels = [map_to_string(prediction) for prediction in predicted_strategies]\n",
    "\n",
    "# Adding predictions to the test dataset\n",
    "test_data['Strategy'] = predicted_labels\n",
    "final = test_data[['id', 'Date', 'Close', 'Strategy']]\n",
    "final.to_csv('prediction.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.312534,
   "end_time": "2023-11-01T18:10:53.949363",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-01T18:10:39.636829",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
